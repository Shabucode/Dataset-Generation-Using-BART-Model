# -*- coding: utf-8 -*-
"""SURREYCCBART.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uG_uqPEl-D-FT_jl0-rxYIf-OR4zzri3

### BART MODEL FOR CREATING DATASET
"""

!pip install transformers

!pip install python-docx

import docx
import csv
from transformers import pipeline, BartTokenizer, BartForConditionalGeneration



# Load the word document
document = docx.Document("1Common_mental_health_problem.docx")

# Extract the text from the document
text = " ".join([paragraph.text for paragraph in document.paragraphs])

# Preprocess the text (cleaning, sentence tokenization, etc.)

chunks = list([paragraph.text for paragraph in document.paragraphs])

# Define the chunk size (number of words per chunk)
#chunk_size = 500

# Chunk the text into smaller parts
#chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

# Initialize the BART model and tokenizer
#model_name = "facebook/bart-base"
#tokenizer = BartTokenizer.from_pretrained(model_name)

#model = BartForConditionalGeneration.from_pretrained(model_name)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("voidful/bart-eqg-question-generator")
model = AutoModelForSeq2SeqLM.from_pretrained("voidful/bart-eqg-question-generator")

# Generate questions using chunking and BART model
question_generator = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
qa_pairs = []
for chunk in chunks:
    question = question_generator(chunk, max_length=64, num_return_sequences=1, num_beams=5)
    answer = chunk
    qa_pairs.append({
        "question": question[0]["generated_text"],
        "answer": answer
    })

# Store the question-answer pairs in a CSV file
csv_file = "qa_pairs.csv"

print(qa_pairs)

fieldnames = ["question", "answer"]
with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.DictWriter(file, fieldnames=fieldnames)
    writer.writeheader()
    #for qa_pair in qa_pairs:
    writer.writerows(qa_pairs)

# Print the generated questions and answers
for qa_pair in qa_pairs:
    print("Question:", qa_pair["question"])
    print("Answer:", qa_pair["answer"])
    print("------------")



import docx
import csv
from transformers import pipeline, BartTokenizer, BartForConditionalGeneration

# Load the word document
document = docx.Document("1Common_mental_health_problem.docx")

# Extract the text from the document
text = " ".join([paragraph.text for paragraph in document.paragraphs])

# Preprocess the text (cleaning, sentence tokenization, etc.)

chunks = list([paragraph.text for paragraph in document.paragraphs])

# Define the chunk size (number of words per chunk)
#chunk_size = 500

# Chunk the text into smaller parts
#chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

# Initialize the BART model and tokenizer
#model_name = "facebook/bart-base"
#tokenizer = BartTokenizer.from_pretrained(model_name)

#model = BartForConditionalGeneration.from_pretrained(model_name)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("voidful/bart-eqg-question-generator")
model = AutoModelForSeq2SeqLM.from_pretrained("voidful/bart-eqg-question-generator")

# Generate questions using chunking and BART model
question_generator = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
qa_pairs = []
for chunk in chunks:
    question = question_generator(chunk, max_length=64, num_return_sequences=1, num_beams=5)
    answer = chunk
    qa_pairs.append({
        "question": question[0]["generated_text"],
        "answer": answer
    })

# Store the question-answer pairs in a CSV file
csv_file = "qa_pairs.csv"